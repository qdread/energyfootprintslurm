---
title: "GHG footprint of Slurm job"
author: "Quentin D. Read"
date: "10/27/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## How much time was saved from code optimization?

The unoptimized code took 2756 seconds (roughly `r round(2756/60)` minutes) to run 100 iterations across 8 cores (processors). Assuming that each processor was used for the entire time, that's `r 2756*8` processor-seconds, which comes out to `r 2756*8/100` seconds per iteration. In minutes:

```{r}
unopt_run_time <- 2756
n_iter <- 100
n_cores <- 8

unopt_seconds_run <- unopt_run_time * n_cores
unopt_seconds_per_iter <- unopt_seconds_run / n_iter
unopt_minutes_per_iter <- unopt_seconds_per_iter/60

paste(floor(unopt_minutes_per_iter), 'minutes', 
      round((unopt_minutes_per_iter - floor(unopt_minutes_per_iter))*60), 'seconds')
```

Running 10,000 iterations, that results in approximately `r round(unopt_seconds_per_iter * 10000/3600)` processor-hours, which is over `r floor(unopt_seconds_per_iter * 10000/3600/24)` processor-days!

The optimized code had an extra setup script that ran once on a single processor for 1066 seconds (roughly `r round(1066/60)` minutes). Then, 100 iterations were run in just 29 seconds! That would be `r round(29*8/100, 2)` seconds per iteration, or `r 1066 + 10000*29*8/100` processor-seconds (around `r round(1066/3600 + 10000*29*8/100/3600)` processor-hours) to run the entire 10,000 iterations, with setup included.

```{r}
opt_setup_time <- 1066
opt_run_time <- 29
n_iter <- 100
n_cores <- 8

opt_seconds_run <- opt_run_time * n_cores
opt_seconds_per_iter <- opt_seconds_run / n_iter

paste(signif(opt_seconds_per_iter, 3), 'seconds')
```

So the amount of time saved is about `r signif(unopt_seconds_per_iter * 10000/3600 - (1066/3600 + opt_seconds_per_iter * 10000/3600), 3)` hours!

## GHG intensity of electricity generation in Maryland

We need two data sources from the US Energy Information Administration: the total emissions generated by electricity generation by state, and the total electricity generated by state. Dividing the total emissions by total electricity will give the GHG intensity per unit electricity generated. Despite the fact that EIA is notorious for fancily formatted spreadsheets that are hard to read into R, these spreadsheets are actually reasonably clean.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readxl)

clean_names <- function(x) {
  x <- tolower(x)
  x <- gsub("\n", "_", x)
  x <- gsub(" ", "_", x)
  gsub("\\(|\\)", "", x)
}

emission_annual <- read_xls('emission_annual.xls', sheet = 1) %>% 
  rename_with(clean_names)
annual_generation_state <-read_xls('annual_generation_state.xls', sheet = 1, skip = 1) %>% 
  rename_with(clean_names)
```

What is the most recent year available for Maryland?

```{r}
emission_annual %>%
  filter(state %in% 'MD', year %in% max(year)) 

annual_generation_state %>%
  filter(state %in% 'MD', year %in% max(year))
```

Let's just use the "Total Electric Power Industry" rows because I'm not sure how the subcategories match up between the two.

```{r}
md_emission <- emission_annual %>%
  filter(state %in% 'MD', year %in% max(year), 
         producer_type %in% 'Total Electric Power Industry') 

md_generation <- annual_generation_state %>%
  filter(state %in% 'MD', year %in% max(year), 
         type_of_producer %in% 'Total Electric Power Industry')
```

Since we don't know the mix of sources used to power Park Place versus UMD, we will have to just stick with the total. We can ignore the SO2 and NOx columns. We have tonnes per MWh, which is equivalent to kg per kWh.

```{r}
total_emissions <- with(md_emission, co2_metric_tons[energy_source %in% 'All Sources'])
total_generation <- with(md_generation, generation_megawatthours[energy_source %in% 'Total'])

total_emissions / total_generation
```

So about 333 grams of CO<sub>2</sub> are released to provide 1 kWh of electricity in Maryland, averaging across all possible modes.

## How much electricity does it take to run the Slurm cluster?

We found a number of different sources for this. It seems like the characteristics of the hardware have a pretty big impact on how much power it draws, which makes sense. How computationally intensive the job is doesn't have as big of an impact. Based on the different sources, it looks like a single Slurm processor core draws somewhere between 15 and 50 W when running a job (based on estimates of 60-200 W for a 4-core processor). I'm assuming a triangular distribution between 15 and 50, with a peak in the middle at 33 W. (for now)

## What's the CO2 footprint of the two jobs? And the difference between them?

That is easily calculated from the unoptimized and optimized run times, and the range of values for power consumption.

```{r}
unopt_time <- unopt_seconds_per_iter * 10000
unopt_energy <- unopt_time/3600 * c(15, 32.5, 50)/1000 # in kWh
unopt_co2 <- unopt_energy * total_emissions / total_generation

opt_time <- 1066 + opt_seconds_per_iter * 10000
opt_energy <- opt_time/3600 * c(15, 32.5, 50)/1000 # in kWh
opt_co2 <- opt_energy * total_emissions / total_generation

# Point estimate for kg CO2 saved.
unopt_co2[2] - opt_co2[2]
```

Our point estimate is about `r signif(unopt_co2[2] - opt_co2[2], 3)` kg CO2 saved!

## Put that in terms I can understand

Let's compare the CO2 footprint of the energy saved with the CO2 footprint of some other activities:

- Driving a passenger car
- Streaming Netflix
- Eating a hamburger

For the car, we can use EPA's number of 404 g/mile (for now). For Netflix, it's a bit more uncertain since there are a number of sources of emissions: the data center, the data transmission, and the device you are streaming on. The CO2 footprint of Wifi versus 4G is pretty different (much higher for 4G). Obviously larger-screen devices have a bigger footprint. Let's use the weighted average of 70 g per hour across all modes of data transmission and whatever device you might use.

Assuming EPA's number for GHG intensity of driving a passenger car, the amount of CO2 saved would get us `r signif((unopt_co2[2] - opt_co2[2])/0.404, 3)` miles or `r signif(units::set_units((unopt_co2[2] - opt_co2[2])/0.404, 'mi') %>% units::set_units('km'), 3)` kilometers in a car! That would get you from Annapolis across the Bay Bridge and to the far side of Kent Island.

Using the weighted average across devices, with the CO2 saved from optimization, you would be able to kick back and stream Netflix for `r signif((unopt_co2[2] - opt_co2[2])/0.07, 3)` hours, or about `r round((unopt_co2[2] - opt_co2[2])/0.07/24)` days straight! That's almost exactly enough to watch every episode of "Great British Baking Show" ever recorded.

You'd only be able to produce `r signif((unopt_co2[2] - opt_co2[2])/4, 2)` Big Macs with that amount of CO2, though, which is probably a bad thing for several reasons.

## Data sources